import argparse
import requests
from bs4 import BeautifulSoup
import csv
from config import domains
import time
from loguru import logger
import pandas as pd
import dateutil.parser
import numpy as np

is_first = True
lifetime_values = []
rows = 0
logger.add('Detection of phishing URLs.log',
           format='{level} {time:MM-DD HH:mm:ss.SSS} {name}:{function}:{line} {message}',
           level='INFO',
           backtrace=True, diagnose=True, enqueue=True, colorize=True)

parser = argparse.ArgumentParser(description='Whois')
VALUES = ('domain name', 'domain', 'updated date', 'modified', 'creation date', 'created',
          'registrar registration expiration date', 'expires', 'paid-till', 'registry expiry date', 'registrar',
          'registrant', 'registrant name', 'person', 'registrant organization', 'organization', 'registrant country',
          'country')


def days_between(d1, d2):
    d1 = dateutil.parser.parse(d1)
    d2 = dateutil.parser.parse(d2)
    return abs((d2 - d1).days)


def get_domain_info(domain):
    domain_without_tld, tld = domain.split(".", 1)
    request = requests.post('https://www.eurodns.com/actions/domainApi/request/getWhois',
                            data={'tld_name': f'{tld}', 'tld_unicodeName': f'{tld}',
                                  'domainWithoutTld': f'{domain_without_tld}', 'language': 'en', 'captchaResponse': ''})

    if request.status_code != 200:
        return
    response_html = BeautifulSoup(request.text, 'html.parser')
    if (pre := response_html.find('pre')) is None:
        parser.exit(1, 'No data available :^(')
    return pre.text.split('\\n')


def parse_domain_info(text):
    save = {}
    for raw in text:
        value = raw.split(': ')
        if value[0].lower() in VALUES:
            save[f'{value[0]}'] = value[1]
    return save


def save_to_csv(domain, file, data):
    global is_first
    with open(file, 'a', newline='') as f:
        writer = csv.writer(f)
        if is_first:
            writer.writerow(data.keys())
            is_first = False
        writer.writerow(data.values())
        logger.success(f'Data for {domain} has been extracted ')


def calculate_lifetime(rows, save_filename):
    df = pd.read_csv(save_filename)
    for it in range(rows):
        lifetime(it, save_filename)
    df['The lifetime of a domain (years)'] = np.resize(lifetime_values, len(df))
    df.to_csv(save_filename, index=False)


def lifetime(i, save_filename):
    df = pd.read_csv(save_filename)
    df.to_csv(save_filename, index=False)
    df = pd.read_csv(save_filename)
    res = int(days_between(df['Registrar Registration Expiration Date'][i], df['Creation Date'][i]) / 365)
    lifetime_values.append(res)
    return lifetime_values


def main():
    save_filename = '1clear_urls.csv'
    for domain in sorted(domains):
        global rows
        successful = False
        while not successful:
            try:
                domain_info = get_domain_info(domain)
                save_data = parse_domain_info(domain_info)
                save_to_csv(domain, save_filename, save_data)
                successful = True
                rows += 1
            except Exception as _e:
                logger.error(f"Error parsing a {domain}: {_e}")
                logger.warning(f"Waiting 10 seconds ..")
                time.sleep(10)
    calculate_lifetime(rows, save_filename)


if __name__ == '__main__':
    main()
